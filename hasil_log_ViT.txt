Using GPU: NVIDIA GeForce RTX 3050 Laptop GPU
Using image size: 224x224
Loading CIFAR-10 datasets...
Train dataset size: 40000
Validation dataset size: 10000
Creating label encoder...
Number of classes: 10
Classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
Label to index mapping: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9}

Initializing ViT-Small model...
Loaded ViT-Small pretrained model
Fine-tuning strategy:
- Frozen: All backbone layers (patch embed, position embed, transformer blocks)
- Trainable: Only classification head (10 classes)
ViTSmall(
  (model): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): Identity()
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (patch_drop): Identity()
    (norm_pre): Identity()
    (blocks): Sequential(
      (0): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (1): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (2): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (3): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (4): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (5): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (6): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (7): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (8): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (9): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (10): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (11): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
    )
    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
    (fc_norm): Identity()
    (head_drop): Dropout(p=0.0, inplace=False)
    (head): Linear(in_features=384, out_features=10, bias=True)
  )
)
Total parameters: 21,669,514
Trainable parameters: 3,850
Frozen parameters: 21,665,664

Starting training with:
- Device: cuda
- Model: ViT-Small (pretrained)
- Image size: 224x224
- Batch size: 128
- Base learning rate: 0.0015
- Warmup epochs: 2
- Number of epochs: 25
- Weight decay: 0.01
- Gradient clipping: 1.0
- Transfer learning: Backbone frozen, head trainable
- Scheduler: Cosine with warmup
--------------------------------------------------------------------------------

Epoch [1/25]
--------------------------------------------------
Batch [0/313], Loss: 3.1452
Batch [10/313], Loss: 0.8661
Batch [20/313], Loss: 0.3525
Batch [30/313], Loss: 0.2623
Batch [40/313], Loss: 0.3340
Batch [50/313], Loss: 0.2322
Batch [60/313], Loss: 0.2661
Batch [70/313], Loss: 0.2502
Batch [80/313], Loss: 0.1489
Batch [90/313], Loss: 0.1841
Batch [100/313], Loss: 0.1373
Batch [110/313], Loss: 0.2376
Batch [120/313], Loss: 0.1293
Batch [130/313], Loss: 0.1183
Batch [140/313], Loss: 0.2123
Batch [150/313], Loss: 0.2563
Batch [160/313], Loss: 0.2074
Batch [170/313], Loss: 0.3326
Batch [180/313], Loss: 0.2787
Batch [190/313], Loss: 0.1946
Batch [200/313], Loss: 0.2575
Batch [210/313], Loss: 0.2583
Batch [220/313], Loss: 0.1589
Batch [230/313], Loss: 0.1259
Batch [240/313], Loss: 0.2047
Batch [250/313], Loss: 0.1591
Batch [260/313], Loss: 0.1750
Batch [270/313], Loss: 0.2127
Batch [280/313], Loss: 0.1576
Batch [290/313], Loss: 0.1619
Batch [300/313], Loss: 0.1924
Batch [310/313], Loss: 0.1496

Epoch 1 Results:
Time: 186.15s
Train - Loss: 0.2785, Acc: 0.9088, F1: 0.9088, Precision: 0.9089, Recall: 0.9088
Val   - Loss: 0.1784, Acc: 0.9380, F1: 0.9383, Precision: 0.9394, Recall: 0.9380
New best model saved! Validation accuracy: 0.9380
--------------------------------------------------------------------------------

Epoch [2/25]
--------------------------------------------------
Batch [0/313], Loss: 0.2577
Batch [10/313], Loss: 0.1584
Batch [20/313], Loss: 0.1187
Batch [30/313], Loss: 0.1035
Batch [40/313], Loss: 0.1203
Batch [50/313], Loss: 0.2478
Batch [60/313], Loss: 0.1349
Batch [70/313], Loss: 0.0922
Batch [80/313], Loss: 0.1781
Batch [90/313], Loss: 0.1315
Batch [100/313], Loss: 0.1749
Batch [110/313], Loss: 0.1265
Batch [120/313], Loss: 0.1050
Batch [130/313], Loss: 0.2323
Batch [140/313], Loss: 0.1888
Batch [150/313], Loss: 0.1995
Batch [160/313], Loss: 0.3086
Batch [170/313], Loss: 0.2070
Batch [180/313], Loss: 0.1293
Batch [190/313], Loss: 0.0846
Batch [200/313], Loss: 0.1477
Batch [210/313], Loss: 0.1749
Batch [220/313], Loss: 0.1477
Batch [230/313], Loss: 0.2085
Batch [240/313], Loss: 0.1387
Batch [250/313], Loss: 0.1534
Batch [260/313], Loss: 0.1563
Batch [270/313], Loss: 0.0629
Batch [280/313], Loss: 0.1669
Batch [290/313], Loss: 0.1927
Batch [300/313], Loss: 0.1176
Batch [310/313], Loss: 0.1197

Epoch 2 Results:
Time: 193.01s
Train - Loss: 0.1683, Acc: 0.9439, F1: 0.9439, Precision: 0.9439, Recall: 0.9439
Val   - Loss: 0.1610, Acc: 0.9444, F1: 0.9446, Precision: 0.9451, Recall: 0.9444
New best model saved! Validation accuracy: 0.9444
--------------------------------------------------------------------------------

Epoch [3/25]
--------------------------------------------------
Batch [0/313], Loss: 0.1348
Batch [10/313], Loss: 0.1192
Batch [20/313], Loss: 0.0840
Batch [30/313], Loss: 0.1331
Batch [40/313], Loss: 0.1823
Batch [50/313], Loss: 0.2613
Batch [60/313], Loss: 0.1932
Batch [70/313], Loss: 0.1169
Batch [80/313], Loss: 0.1556
Batch [90/313], Loss: 0.1370
Batch [100/313], Loss: 0.0704
Batch [110/313], Loss: 0.1788
Batch [120/313], Loss: 0.2113
Batch [130/313], Loss: 0.0887
Batch [140/313], Loss: 0.1316
Batch [150/313], Loss: 0.2425
Batch [160/313], Loss: 0.1842
Batch [170/313], Loss: 0.1182
Batch [180/313], Loss: 0.1675
Batch [190/313], Loss: 0.1358
Batch [200/313], Loss: 0.1412
Batch [210/313], Loss: 0.0887
Batch [220/313], Loss: 0.1565
Batch [230/313], Loss: 0.1432
Batch [240/313], Loss: 0.1765
Batch [250/313], Loss: 0.0685
Batch [260/313], Loss: 0.1012
Batch [270/313], Loss: 0.1363
Batch [280/313], Loss: 0.0900
Batch [290/313], Loss: 0.1625
Batch [300/313], Loss: 0.1683
Batch [310/313], Loss: 0.0644

Epoch 3 Results:
Time: 198.54s
Train - Loss: 0.1559, Acc: 0.9466, F1: 0.9466, Precision: 0.9466, Recall: 0.9466
Val   - Loss: 0.1703, Acc: 0.9420, F1: 0.9423, Precision: 0.9447, Recall: 0.9420
No improvement in validation accuracy for 1 epoch(s)
--------------------------------------------------------------------------------

Epoch [4/25]
--------------------------------------------------
Batch [0/313], Loss: 0.2711
Batch [10/313], Loss: 0.1236
Batch [20/313], Loss: 0.1597
Batch [30/313], Loss: 0.0712
Batch [40/313], Loss: 0.0768
Batch [50/313], Loss: 0.1344
Batch [60/313], Loss: 0.1276
Batch [70/313], Loss: 0.1242
Batch [80/313], Loss: 0.1551
Batch [90/313], Loss: 0.1606
Batch [100/313], Loss: 0.1634
Batch [110/313], Loss: 0.1214
Batch [120/313], Loss: 0.2122
Batch [130/313], Loss: 0.1359
Batch [140/313], Loss: 0.1310
Batch [150/313], Loss: 0.1632
Batch [160/313], Loss: 0.1806
Batch [170/313], Loss: 0.1673
Batch [180/313], Loss: 0.1037
Batch [190/313], Loss: 0.0762
Batch [200/313], Loss: 0.1013
Batch [210/313], Loss: 0.1208
Batch [220/313], Loss: 0.1399
Batch [230/313], Loss: 0.1172
Batch [240/313], Loss: 0.0820
Batch [250/313], Loss: 0.0970
Batch [260/313], Loss: 0.1555
Batch [270/313], Loss: 0.1985
Batch [280/313], Loss: 0.1107
Batch [290/313], Loss: 0.1852
Batch [300/313], Loss: 0.1868
Batch [310/313], Loss: 0.1384

Epoch 4 Results:
Time: 201.53s
Train - Loss: 0.1474, Acc: 0.9496, F1: 0.9496, Precision: 0.9497, Recall: 0.9496
Val   - Loss: 0.1579, Acc: 0.9458, F1: 0.9459, Precision: 0.9462, Recall: 0.9458
New best model saved! Validation accuracy: 0.9458
--------------------------------------------------------------------------------

Epoch [5/25]
--------------------------------------------------
Batch [0/313], Loss: 0.1362
Batch [10/313], Loss: 0.0931
Batch [20/313], Loss: 0.1131
Batch [30/313], Loss: 0.1216
Batch [40/313], Loss: 0.1464
Batch [50/313], Loss: 0.2009
Batch [60/313], Loss: 0.1175
Batch [70/313], Loss: 0.1101
Batch [80/313], Loss: 0.1634
Batch [90/313], Loss: 0.1513
Batch [100/313], Loss: 0.0984
Batch [110/313], Loss: 0.0851
Batch [120/313], Loss: 0.1437
Batch [130/313], Loss: 0.1128
Batch [140/313], Loss: 0.0989
Batch [150/313], Loss: 0.1695
Batch [160/313], Loss: 0.1881
Batch [170/313], Loss: 0.1920
Batch [180/313], Loss: 0.0763
Batch [190/313], Loss: 0.1847
Batch [200/313], Loss: 0.1523
Batch [210/313], Loss: 0.2365
Batch [220/313], Loss: 0.2087
Batch [230/313], Loss: 0.1734
Batch [240/313], Loss: 0.1119
Batch [250/313], Loss: 0.0666
Batch [260/313], Loss: 0.1927
Batch [270/313], Loss: 0.1250
Batch [280/313], Loss: 0.1760
Batch [290/313], Loss: 0.1654
Batch [300/313], Loss: 0.1329
Batch [310/313], Loss: 0.1191

Epoch 5 Results:
Time: 202.84s
Train - Loss: 0.1444, Acc: 0.9501, F1: 0.9501, Precision: 0.9501, Recall: 0.9501
Val   - Loss: 0.1586, Acc: 0.9464, F1: 0.9464, Precision: 0.9467, Recall: 0.9464
New best model saved! Validation accuracy: 0.9464
--------------------------------------------------------------------------------

Epoch [6/25]
--------------------------------------------------
Batch [0/313], Loss: 0.1377
Batch [10/313], Loss: 0.1600
Batch [20/313], Loss: 0.1296
Batch [30/313], Loss: 0.1970
Batch [40/313], Loss: 0.0910
Batch [50/313], Loss: 0.1950
Batch [60/313], Loss: 0.0894
Batch [70/313], Loss: 0.2238
Batch [80/313], Loss: 0.1887
Batch [90/313], Loss: 0.1746
Batch [100/313], Loss: 0.0844
Batch [110/313], Loss: 0.1696
Batch [120/313], Loss: 0.1344
Batch [130/313], Loss: 0.0841
Batch [140/313], Loss: 0.2325
Batch [150/313], Loss: 0.0713
Batch [160/313], Loss: 0.1231
Batch [170/313], Loss: 0.0532
Batch [180/313], Loss: 0.0845
Batch [190/313], Loss: 0.1712
Batch [200/313], Loss: 0.1388
Batch [210/313], Loss: 0.1412
Batch [220/313], Loss: 0.1148
Batch [230/313], Loss: 0.1501
Batch [240/313], Loss: 0.1616
Batch [250/313], Loss: 0.1532
Batch [260/313], Loss: 0.1157
Batch [270/313], Loss: 0.1501
Batch [280/313], Loss: 0.1532
Batch [290/313], Loss: 0.1817
Batch [300/313], Loss: 0.2495
Batch [310/313], Loss: 0.0919

Epoch 6 Results:
Time: 202.93s
Train - Loss: 0.1423, Acc: 0.9513, F1: 0.9513, Precision: 0.9513, Recall: 0.9513
Val   - Loss: 0.1687, Acc: 0.9437, F1: 0.9437, Precision: 0.9442, Recall: 0.9437
No improvement in validation accuracy for 1 epoch(s)
--------------------------------------------------------------------------------

Epoch [7/25]
--------------------------------------------------
Batch [0/313], Loss: 0.1170
Batch [10/313], Loss: 0.1544
Batch [20/313], Loss: 0.1352
Batch [30/313], Loss: 0.1303
Batch [40/313], Loss: 0.2125
Batch [50/313], Loss: 0.0737
Batch [60/313], Loss: 0.1215
Batch [70/313], Loss: 0.1646
Batch [80/313], Loss: 0.1361
Batch [90/313], Loss: 0.1654
Batch [100/313], Loss: 0.1221
Batch [110/313], Loss: 0.1684
Batch [120/313], Loss: 0.1400
Batch [130/313], Loss: 0.2132
Batch [140/313], Loss: 0.0789
Batch [150/313], Loss: 0.0803
Batch [160/313], Loss: 0.1595
Batch [170/313], Loss: 0.1649
Batch [180/313], Loss: 0.1198
Batch [190/313], Loss: 0.1080
Batch [200/313], Loss: 0.1249
Batch [210/313], Loss: 0.0797
Batch [220/313], Loss: 0.0987
Batch [230/313], Loss: 0.0793
Batch [240/313], Loss: 0.0832
Batch [250/313], Loss: 0.2325
Batch [260/313], Loss: 0.1092
Batch [270/313], Loss: 0.1610
Batch [280/313], Loss: 0.1332
Batch [290/313], Loss: 0.1058
Batch [300/313], Loss: 0.1442
Batch [310/313], Loss: 0.1507

Epoch 7 Results:
Time: 203.90s
Train - Loss: 0.1404, Acc: 0.9514, F1: 0.9514, Precision: 0.9514, Recall: 0.9514
Val   - Loss: 0.1643, Acc: 0.9468, F1: 0.9469, Precision: 0.9472, Recall: 0.9468
New best model saved! Validation accuracy: 0.9468
--------------------------------------------------------------------------------

Epoch [8/25]
--------------------------------------------------
Batch [0/313], Loss: 0.1174
Batch [10/313], Loss: 0.1677
Batch [20/313], Loss: 0.0651
Batch [30/313], Loss: 0.1191
Batch [40/313], Loss: 0.1380
Batch [50/313], Loss: 0.0787
Batch [60/313], Loss: 0.0887
Batch [70/313], Loss: 0.1117
Batch [80/313], Loss: 0.1480
Batch [90/313], Loss: 0.0953
Batch [100/313], Loss: 0.1730
Batch [110/313], Loss: 0.1294
Batch [120/313], Loss: 0.0286
Batch [130/313], Loss: 0.1066
Batch [140/313], Loss: 0.1492
Batch [150/313], Loss: 0.1709
Batch [160/313], Loss: 0.1811
Batch [170/313], Loss: 0.1747
Batch [180/313], Loss: 0.2506
Batch [190/313], Loss: 0.2129
Batch [200/313], Loss: 0.1256
Batch [210/313], Loss: 0.1687
Batch [220/313], Loss: 0.1551
Batch [230/313], Loss: 0.1162
Batch [240/313], Loss: 0.1153
Batch [250/313], Loss: 0.1576
Batch [260/313], Loss: 0.1185
Batch [270/313], Loss: 0.0391
Batch [280/313], Loss: 0.1232
Batch [290/313], Loss: 0.1123
Batch [300/313], Loss: 0.1059
Batch [310/313], Loss: 0.0479

Epoch 8 Results:
Time: 204.11s
Train - Loss: 0.1379, Acc: 0.9517, F1: 0.9517, Precision: 0.9517, Recall: 0.9517
Val   - Loss: 0.1713, Acc: 0.9452, F1: 0.9454, Precision: 0.9467, Recall: 0.9452
No improvement in validation accuracy for 1 epoch(s)
--------------------------------------------------------------------------------

Epoch [9/25]
--------------------------------------------------
Batch [0/313], Loss: 0.1545
Batch [10/313], Loss: 0.1665
Batch [20/313], Loss: 0.2130
Batch [30/313], Loss: 0.1827
Batch [40/313], Loss: 0.1658
Batch [50/313], Loss: 0.0834
Batch [60/313], Loss: 0.1640
Batch [70/313], Loss: 0.1091
Batch [80/313], Loss: 0.2029
Batch [90/313], Loss: 0.1010
Batch [100/313], Loss: 0.1061
Batch [110/313], Loss: 0.0782
Batch [120/313], Loss: 0.2039
Batch [130/313], Loss: 0.1236
Batch [140/313], Loss: 0.1042
Batch [150/313], Loss: 0.1959
Batch [160/313], Loss: 0.2152
Batch [170/313], Loss: 0.0677
Batch [180/313], Loss: 0.1268
Batch [190/313], Loss: 0.1515
Batch [200/313], Loss: 0.0713
Batch [210/313], Loss: 0.1071
Batch [220/313], Loss: 0.2057
Batch [230/313], Loss: 0.0997
Batch [240/313], Loss: 0.1217
Batch [250/313], Loss: 0.1290
Batch [260/313], Loss: 0.1869
Batch [270/313], Loss: 0.0918
Batch [280/313], Loss: 0.1594
Batch [290/313], Loss: 0.1492
Batch [300/313], Loss: 0.0788
Batch [310/313], Loss: 0.0988

Epoch 9 Results:
Time: 204.76s
Train - Loss: 0.1375, Acc: 0.9529, F1: 0.9529, Precision: 0.9529, Recall: 0.9529
Val   - Loss: 0.1749, Acc: 0.9416, F1: 0.9414, Precision: 0.9423, Recall: 0.9416
No improvement in validation accuracy for 2 epoch(s)
--------------------------------------------------------------------------------

Epoch [10/25]
--------------------------------------------------
Batch [0/313], Loss: 0.1047
Batch [10/313], Loss: 0.0829
Batch [20/313], Loss: 0.1901
Batch [30/313], Loss: 0.0897
Batch [40/313], Loss: 0.0808
Batch [50/313], Loss: 0.1242
Batch [60/313], Loss: 0.1158
Batch [70/313], Loss: 0.1765
Batch [80/313], Loss: 0.1357
Batch [90/313], Loss: 0.1896
Batch [100/313], Loss: 0.1919
Batch [110/313], Loss: 0.0846
Batch [120/313], Loss: 0.1679
Batch [130/313], Loss: 0.1426
Batch [140/313], Loss: 0.1136
Batch [150/313], Loss: 0.0818
Batch [160/313], Loss: 0.1495
Batch [170/313], Loss: 0.1600
Batch [180/313], Loss: 0.0875
Batch [190/313], Loss: 0.1407
Batch [200/313], Loss: 0.0851
Batch [210/313], Loss: 0.1430
Batch [220/313], Loss: 0.1749
Batch [230/313], Loss: 0.0757
Batch [240/313], Loss: 0.2550
Batch [250/313], Loss: 0.1698
Batch [260/313], Loss: 0.1430
Batch [270/313], Loss: 0.1009
Batch [280/313], Loss: 0.1158
Batch [290/313], Loss: 0.1683
Batch [300/313], Loss: 0.1533
Batch [310/313], Loss: 0.1295

Epoch 10 Results:
Time: 205.05s
Train - Loss: 0.1326, Acc: 0.9536, F1: 0.9536, Precision: 0.9536, Recall: 0.9536
Val   - Loss: 0.1741, Acc: 0.9430, F1: 0.9432, Precision: 0.9439, Recall: 0.9430
No improvement in validation accuracy for 3 epoch(s)
--------------------------------------------------------------------------------

Epoch [11/25]
--------------------------------------------------
Batch [0/313], Loss: 0.0794
Batch [10/313], Loss: 0.1045
Batch [20/313], Loss: 0.0728
Batch [30/313], Loss: 0.0990
Batch [40/313], Loss: 0.0383
Batch [50/313], Loss: 0.1645
Batch [60/313], Loss: 0.1649
Batch [70/313], Loss: 0.1016
Batch [80/313], Loss: 0.2564
Batch [90/313], Loss: 0.0781
Batch [100/313], Loss: 0.0341
Batch [110/313], Loss: 0.0975
Batch [120/313], Loss: 0.1477
Batch [130/313], Loss: 0.1379
Batch [140/313], Loss: 0.1299
Batch [150/313], Loss: 0.1671
Batch [160/313], Loss: 0.1197
Batch [170/313], Loss: 0.1261
Batch [180/313], Loss: 0.1441
Batch [190/313], Loss: 0.0625
Batch [200/313], Loss: 0.0768
Batch [210/313], Loss: 0.2459
Batch [220/313], Loss: 0.1570
Batch [230/313], Loss: 0.1832
Batch [240/313], Loss: 0.0742
Batch [250/313], Loss: 0.1471
Batch [260/313], Loss: 0.1495
Batch [270/313], Loss: 0.0707
Batch [280/313], Loss: 0.0828
Batch [290/313], Loss: 0.1331
Batch [300/313], Loss: 0.1391
Batch [310/313], Loss: 0.0600

Epoch 11 Results:
Time: 204.23s
Train - Loss: 0.1341, Acc: 0.9533, F1: 0.9533, Precision: 0.9533, Recall: 0.9533
Val   - Loss: 0.1675, Acc: 0.9469, F1: 0.9469, Precision: 0.9471, Recall: 0.9469
New best model saved! Validation accuracy: 0.9469
--------------------------------------------------------------------------------

Epoch [12/25]
--------------------------------------------------
Batch [0/313], Loss: 0.0517
Batch [10/313], Loss: 0.1368
Batch [20/313], Loss: 0.1102
Batch [30/313], Loss: 0.1740
Batch [40/313], Loss: 0.1666
Batch [50/313], Loss: 0.0705
Batch [60/313], Loss: 0.1403
Batch [70/313], Loss: 0.0816
Batch [80/313], Loss: 0.1212
Batch [90/313], Loss: 0.1967
Batch [100/313], Loss: 0.1397
Batch [110/313], Loss: 0.0382
Batch [120/313], Loss: 0.0582
Batch [130/313], Loss: 0.0855
Batch [140/313], Loss: 0.1213
Batch [150/313], Loss: 0.0740
Batch [160/313], Loss: 0.1631
Batch [170/313], Loss: 0.1822
Batch [180/313], Loss: 0.0977
Batch [190/313], Loss: 0.1083
Batch [200/313], Loss: 0.1915
Batch [210/313], Loss: 0.1196
Batch [220/313], Loss: 0.2300
Batch [230/313], Loss: 0.0650
Batch [240/313], Loss: 0.1373
Batch [250/313], Loss: 0.1532
Batch [260/313], Loss: 0.0549
Batch [270/313], Loss: 0.2116
Batch [280/313], Loss: 0.2073
Batch [290/313], Loss: 0.1025
Batch [300/313], Loss: 0.2040
Batch [310/313], Loss: 0.0913

Epoch 12 Results:
Time: 204.92s
Train - Loss: 0.1319, Acc: 0.9540, F1: 0.9540, Precision: 0.9541, Recall: 0.9540
Val   - Loss: 0.1779, Acc: 0.9424, F1: 0.9425, Precision: 0.9431, Recall: 0.9424
No improvement in validation accuracy for 1 epoch(s)
--------------------------------------------------------------------------------

Epoch [13/25]
--------------------------------------------------
Batch [0/313], Loss: 0.1507
Batch [10/313], Loss: 0.1276
Batch [20/313], Loss: 0.1186
Batch [30/313], Loss: 0.1380
Batch [40/313], Loss: 0.1251
Batch [50/313], Loss: 0.1446
Batch [60/313], Loss: 0.1027
Batch [70/313], Loss: 0.1210
Batch [80/313], Loss: 0.0890
Batch [90/313], Loss: 0.0982
Batch [100/313], Loss: 0.1459
Batch [110/313], Loss: 0.0803
Batch [120/313], Loss: 0.1007
Batch [130/313], Loss: 0.0839
Batch [140/313], Loss: 0.1496
Batch [150/313], Loss: 0.1874
Batch [160/313], Loss: 0.1366
Batch [170/313], Loss: 0.1895
Batch [180/313], Loss: 0.1524
Batch [190/313], Loss: 0.1372
Batch [200/313], Loss: 0.1262
Batch [210/313], Loss: 0.1814
Batch [220/313], Loss: 0.0991
Batch [230/313], Loss: 0.0952
Batch [240/313], Loss: 0.1906
Batch [250/313], Loss: 0.1655
Batch [260/313], Loss: 0.1588
Batch [270/313], Loss: 0.0899
Batch [280/313], Loss: 0.2109
Batch [290/313], Loss: 0.0775
Batch [300/313], Loss: 0.1016
Batch [310/313], Loss: 0.2850

Epoch 13 Results:
Time: 204.87s
Train - Loss: 0.1323, Acc: 0.9537, F1: 0.9537, Precision: 0.9538, Recall: 0.9537
Val   - Loss: 0.1785, Acc: 0.9426, F1: 0.9428, Precision: 0.9434, Recall: 0.9426
No improvement in validation accuracy for 2 epoch(s)
--------------------------------------------------------------------------------

Epoch [14/25]
--------------------------------------------------
Batch [0/313], Loss: 0.1246
Batch [10/313], Loss: 0.1330
Batch [20/313], Loss: 0.1213
Batch [30/313], Loss: 0.1058
Batch [40/313], Loss: 0.1647
Batch [50/313], Loss: 0.1527
Batch [60/313], Loss: 0.1513
Batch [70/313], Loss: 0.0589
Batch [80/313], Loss: 0.1912
Batch [90/313], Loss: 0.1061
Batch [100/313], Loss: 0.0653
Batch [110/313], Loss: 0.2069
Batch [120/313], Loss: 0.1005
Batch [130/313], Loss: 0.1291
Batch [140/313], Loss: 0.1722
Batch [150/313], Loss: 0.1058
Batch [160/313], Loss: 0.1427
Batch [170/313], Loss: 0.1004
Batch [180/313], Loss: 0.1008
Batch [190/313], Loss: 0.1413
Batch [200/313], Loss: 0.1726
Batch [210/313], Loss: 0.2125
Batch [220/313], Loss: 0.1490
Batch [230/313], Loss: 0.1102
Batch [240/313], Loss: 0.0905
Batch [250/313], Loss: 0.1351
Batch [260/313], Loss: 0.2637
Batch [270/313], Loss: 0.1065
Batch [280/313], Loss: 0.1978
Batch [290/313], Loss: 0.0711
Batch [300/313], Loss: 0.1279
Batch [310/313], Loss: 0.1654

Epoch 14 Results:
Time: 204.82s
Train - Loss: 0.1327, Acc: 0.9537, F1: 0.9537, Precision: 0.9537, Recall: 0.9537
Val   - Loss: 0.1745, Acc: 0.9444, F1: 0.9446, Precision: 0.9453, Recall: 0.9444
No improvement in validation accuracy for 3 epoch(s)
--------------------------------------------------------------------------------

Epoch [15/25]
--------------------------------------------------
Batch [0/313], Loss: 0.0998
Batch [10/313], Loss: 0.1145
Batch [20/313], Loss: 0.1267
Batch [30/313], Loss: 0.1861
Batch [40/313], Loss: 0.2256
Batch [50/313], Loss: 0.1163
Batch [60/313], Loss: 0.1677
Batch [70/313], Loss: 0.2118
Batch [80/313], Loss: 0.0526
Batch [90/313], Loss: 0.1527
Batch [100/313], Loss: 0.1374
Batch [110/313], Loss: 0.1944
Batch [120/313], Loss: 0.1404
Batch [130/313], Loss: 0.0997
Batch [140/313], Loss: 0.1866
Batch [150/313], Loss: 0.1142
Batch [160/313], Loss: 0.1712
Batch [170/313], Loss: 0.1493
Batch [180/313], Loss: 0.0603
Batch [190/313], Loss: 0.1733
Batch [200/313], Loss: 0.0894
Batch [210/313], Loss: 0.1608
Batch [220/313], Loss: 0.0487
Batch [230/313], Loss: 0.3291
Batch [240/313], Loss: 0.1261
Batch [250/313], Loss: 0.1692
Batch [260/313], Loss: 0.0573
Batch [270/313], Loss: 0.0456
Batch [280/313], Loss: 0.2605
Batch [290/313], Loss: 0.1316
Batch [300/313], Loss: 0.1416
Batch [310/313], Loss: 0.1308

Epoch 15 Results:
Time: 204.95s
Train - Loss: 0.1333, Acc: 0.9529, F1: 0.9529, Precision: 0.9529, Recall: 0.9529
Val   - Loss: 0.1844, Acc: 0.9415, F1: 0.9412, Precision: 0.9419, Recall: 0.9415
No improvement in validation accuracy for 4 epoch(s)
--------------------------------------------------------------------------------

Epoch [16/25]
--------------------------------------------------
Batch [0/313], Loss: 0.0996
Batch [10/313], Loss: 0.1236
Batch [20/313], Loss: 0.1379
Batch [30/313], Loss: 0.1366
Batch [40/313], Loss: 0.0808
Batch [50/313], Loss: 0.2118
Batch [60/313], Loss: 0.1062
Batch [70/313], Loss: 0.1430
Batch [80/313], Loss: 0.1002
Batch [90/313], Loss: 0.1370
Batch [100/313], Loss: 0.1404
Batch [110/313], Loss: 0.0859
Batch [120/313], Loss: 0.1263
Batch [130/313], Loss: 0.1633
Batch [140/313], Loss: 0.0878
Batch [150/313], Loss: 0.1946
Batch [160/313], Loss: 0.1849
Batch [170/313], Loss: 0.0939
Batch [180/313], Loss: 0.0638
Batch [190/313], Loss: 0.1696
Batch [200/313], Loss: 0.2330
Batch [210/313], Loss: 0.1038
Batch [220/313], Loss: 0.1196
Batch [230/313], Loss: 0.2567
Batch [240/313], Loss: 0.1124
Batch [250/313], Loss: 0.0604
Batch [260/313], Loss: 0.1527
Batch [270/313], Loss: 0.2221
Batch [280/313], Loss: 0.1544
Batch [290/313], Loss: 0.0744
Batch [300/313], Loss: 0.1515
Batch [310/313], Loss: 0.0998

Epoch 16 Results:
Time: 206.71s
Train - Loss: 0.1309, Acc: 0.9549, F1: 0.9549, Precision: 0.9549, Recall: 0.9549
Val   - Loss: 0.1767, Acc: 0.9440, F1: 0.9440, Precision: 0.9443, Recall: 0.9440
No improvement in validation accuracy for 5 epoch(s)

Early stopping triggered! No improvement for 5 epochs.
Best validation accuracy: 0.9469

Training completed!
Best validation accuracy: 0.9469

Grafik training telah disimpan sebagai '/home/manix/Documents/Semester 7/DeepLearn/Tugas-2/hasil_training_ViT.png'

Loading best model for final evaluation...

================================================================================
FINAL EVALUATION RESULTS
================================================================================
Final Validation Metrics:
Accuracy:  0.9469
F1-Score:  0.9469
Precision: 0.9471
Recall:    0.9469

Detailed Classification Report:
--------------------------------------------------
              precision    recall  f1-score   support

           0       0.96      0.97      0.96      1000
           1       0.97      0.96      0.97      1000
           2       0.96      0.93      0.95      1000
           3       0.88      0.88      0.88      1000
           4       0.94      0.95      0.94      1000
           5       0.90      0.92      0.91      1000
           6       0.95      0.98      0.96      1000
           7       0.98      0.95      0.97      1000
           8       0.97      0.98      0.97      1000
           9       0.96      0.96      0.96      1000

    accuracy                           0.95     10000
   macro avg       0.95      0.95      0.95     10000
weighted avg       0.95      0.95      0.95     10000


Generating confusion matrix...
Confusion matrix telah disimpan sebagai '/home/manix/Documents/Semester 7/DeepLearn/Tugas-2/confusion_matrix_vit.png'

================================================================================
INFERENCE TIME MEASUREMENT
================================================================================
Warming up GPU...
Measuring inference time...

Hardware: GPU: NVIDIA GeForce RTX 3050 Laptop GPU (3.7 GB)
Total images processed: 10000
Total inference time: 41.050 seconds
Average time per image: 4.10 ms
Throughput: 243.61 images/second
Average batch time: 0.4917 Â± 0.0495 seconds

Best model saved as: /home/manix/Documents/Semester 7/DeepLearn/Tugas-2/best_vit_model.pth

Model Summary:
- Architecture: ViT-Small with ImageNet pretraining
- Transfer Learning: Frozen backbone + trainable classifier
- Input size: 224x224x3
- Output classes: 10
- Total parameters: 21,669,514
- Trainable parameters: 3,850
